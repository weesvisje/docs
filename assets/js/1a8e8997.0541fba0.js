(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[4695],{14516:(e,t,a)=>{"use strict";a.r(t),a.d(t,{ImageHolder:()=>h,contentTitle:()=>o,default:()=>c,frontMatter:()=>s,metadata:()=>r,toc:()=>l});var n=a(87462),i=(a(67294),a(3905));a(78561);const s={title:"Big Peer"},o=void 0,r={unversionedId:"how-it-works/big-peer",id:"how-it-works/big-peer",isDocsHomePage:!1,title:"Big Peer",description:"export function ImageHolder(props) {",source:"@site/docs/common/how-it-works/big-peer.md",sourceDirName:"how-it-works",slug:"/how-it-works/big-peer",permalink:"/common/how-it-works/big-peer",editUrl:"https://github.com/getditto/docs/edit/main/docs/common/how-it-works/big-peer.md",tags:[],version:"current",frontMatter:{title:"Big Peer"},sidebar:"defaultSidebar",previous:{title:"Certificate-Based Security",permalink:"/common/how-it-works/certificate-security"},next:{title:"Troubleshooting",permalink:"/common/troubleshooting"}},l=[{value:"Why Did You Make It?",id:"why-did-you-make-it",children:[],level:2},{value:"How Does It Work?",id:"how-does-it-work",children:[{value:"Ditto CRDTs",id:"ditto-crdts",children:[],level:3},{value:"Ditto Mesh Replication",id:"ditto-mesh-replication",children:[],level:3},{value:"Apps and Collections",id:"apps-and-collections",children:[],level:3},{value:"Causally Consistent Transactions",id:"causally-consistent-transactions",children:[],level:3},{value:"PaRiS - UST",id:"paris---ust",children:[{value:"Replicas",id:"replicas",children:[],level:4},{value:"Versions",id:"versions",children:[],level:4},{value:"Partitions / Shards",id:"partitions--shards",children:[],level:4},{value:"Non-Blocking Reads",id:"non-blocking-reads",children:[],level:4},{value:"Read Your Own Writes",id:"read-your-own-writes",children:[],level:4}],level:3},{value:"The Log",id:"the-log",children:[],level:3},{value:"Storage Nodes",id:"storage-nodes",children:[{value:"Gossip - UST",id:"gossip---ust",children:[],level:4},{value:"Gossip - Garbage Collection",id:"gossip---garbage-collection",children:[],level:4},{value:"Reading and Read Transactions",id:"reading-and-read-transactions",children:[],level:4},{value:"Cluster Configurations: Who owns what?",id:"cluster-configurations-who-owns-what",children:[],level:4},{value:"Random Slicing",id:"random-slicing",children:[],level:4},{value:"Interval Maps - Missed Transactions - Backfill",id:"interval-maps---missed-transactions---backfill",children:[],level:4}],level:3}],level:2},{value:"Transitions",id:"transitions",children:[{value:"Backfill, again",id:"backfill-again",children:[],level:3},{value:"Routing, and a UST per-Configuration",id:"routing-and-a-ust-per-configuration",children:[],level:3}],level:2},{value:"Handling Failure",id:"handling-failure",children:[{value:"Bad Nodes",id:"bad-nodes",children:[],level:3},{value:"Missed/Lost Data",id:"missedlost-data",children:[],level:3},{value:"Subscription Servers",id:"subscription-servers",children:[{value:"Document Cache",id:"document-cache",children:[],level:4}],level:3}],level:2},{value:"What Next?",id:"what-next",children:[{value:"CDC",id:"cdc",children:[],level:3},{value:"Time Series",id:"time-series",children:[],level:3},{value:"HTTP API",id:"http-api",children:[],level:3},{value:"Eventual Consistency Mode",id:"eventual-consistency-mode",children:[],level:3},{value:"Improved Queries and Indexing",id:"improved-queries-and-indexing",children:[],level:3},{value:"Summary",id:"summary",children:[],level:3}],level:2}];function h(e){return(0,i.kt)("div",{style:{padding:"2rem",margin:"2rem",borderRadius:"8px",background:"white"}},e.children)}const d={toc:l,ImageHolder:h};function c(e){let{components:t,...s}=e;return(0,i.kt)("wrapper",(0,n.Z)({},d,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Ditto's distributed database architecture is a composition of Small Peers and Big Peers. Small peers are predominantly used to synchronize data across web, mobile, desktop, and IoT apps where storage, RAM, and CPU resources are generally static and unchangeable. For example, if you were to buy an iPhone with 256 Gigabytes of storage, you are pretty much stuck with this size unless you buy another iPhone."),(0,i.kt)("p",null,"Conversely, Big Peers are database peers which live in the cloud and are capable of sharding or partitioning. When they sync with small peers, they look like any other peer. However, a Big Peer can be split across multiple virtual or physical nodes allowing for both horizontal and vertical scaling of resources as your application demands grow."),(0,i.kt)("p",null,"The Big Peer fits into Ditto's vision of syncing data, anywhere. Big Peer is cloud-ready, multi-tenant, highly available, fault\ntolerant, offers causally consistent transactions, and works seamlessly with Small Peer devices."),(0,i.kt)("p",null,"For reference:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Web, iOS, Android, Raspberry Pi, Desktop, and some server side apps \ud83d\udc49 ",(0,i.kt)("strong",{parentName:"li"},"Small Peer")),(0,i.kt)("li",{parentName:"ul"},"Ditto Cloud \ud83d\udc49 ",(0,i.kt)("strong",{parentName:"li"},"Big Peer"))),(0,i.kt)("h2",{id:"why-did-you-make-it"},"Why Did You Make It?"),(0,i.kt)("p",null,"Even with the Small Peer's wireless mesh networking capabilities, some pair of devices may not be able to\nexchange data. Maybe the devices are miles apart, or they are never online at the same time. That is where Big Peer fits in. The Big Peer is a database that Small Peer devices can sync with to propagate changes across disconnected meshes, and even back to the enterprise. Often databases are used as channels, which is also one of Big Peer's purposes."),(0,i.kt)("p",null,"There exist many distributed databases, but Big Peer is specifically designed for Ditto: It stores Ditto's ",(0,i.kt)("a",{parentName:"p",href:"../how-it-works/crdt"},"CRDTs"),' by default; it can store and merge Ditto CRDT Diffs; it "speaks" Ditto\'s mesh replication protocol, meaning it appears as just another peer to Ditto mesh devices; and it provides causally consistent transactions.'),(0,i.kt)("h2",{id:"how-does-it-work"},"How Does It Work?"),(0,i.kt)("p",null,'Big Peer is made up of core storage nodes which make a distributed database, and\nsoft-state satellite API nodes, called Subscription Servers, that are also\ncaches of data and replicate with Small Peer clients as a "Big Peer".'),(0,i.kt)("p",null,"The following sections go into detail about what properties and\nfeatures Big Peer has, and how we achieve those properties, leveraging\nour experience building and shipping distributed databases, and\ncurrent computer science systems research."),(0,i.kt)("p",null,"The following drawing is a rough overview of the architecture."),(0,i.kt)(h,{mdxType:"ImageHolder"},(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Big Peer Overview",src:a(83081).Z}))),(0,i.kt)("h3",{id:"ditto-crdts"},"Ditto CRDTs"),(0,i.kt)("p",null,"The core data type in Ditto is the CRDT. It is documented in detail\n",(0,i.kt)("a",{parentName:"p",href:"../how-it-works/crdt"},"here"),". Understanding some\nof how CRDTs work helps understand the concepts below. It is\nenough to know that if the same CRDT is modified by multiple Ditto\nmesh Small Peer devices concurrently there is a way to deterministically\n",(0,i.kt)("em",{parentName:"p"},"merge")," the conflicting versions into a single meaningful value."),(0,i.kt)("h3",{id:"ditto-mesh-replication"},"Ditto Mesh Replication"),(0,i.kt)("p",null,"This is also covered in ",(0,i.kt)("a",{parentName:"p",href:"../how-it-works/mesh-network"},"other documents"),'. All we need know\nhere is that Small Peer devices replicate with Big Peer by sending\nCRDT Documents and CRDT Diffs to Big Peer\'s Subscription Server API, and\nreceive in return Documents and Diffs that they are subscribed to. A\nsubscription is a query, for example "All red cars in the vehicles\ncollection."'),(0,i.kt)("p",null,"Thanks to the Ditto replication protocol, all Documents and Diffs that\nthe client needs to send/receive to/from Big Peer appear to arrive\natomically, as though in a transaction."),(0,i.kt)("h3",{id:"apps-and-collections"},"Apps and Collections"),(0,i.kt)("p",null,"An application is the consistency boundary for Big Peer. An application\nis registered via the ",(0,i.kt)("a",{parentName:"p",href:"https://portal.ditto.live/"},"Portal"),". An application is uniquely\nidentified via association with a UUID. Queries, Subscriptions, and\nTransactions are all scoped by application. Within an application are\nCollections. Theses are somewhat like tables, where associated\nDocuments can be stored. Big Peer supports transactions within an Application, including across\nCollections."),(0,i.kt)("h3",{id:"causally-consistent-transactions"},"Causally Consistent Transactions"),(0,i.kt)("p",null,"Given the existence of the CAP theorem, which posits a fundamental trade off in\ndistributed systems between Consistency and Availability in a world of\nasynchronous networks, Causal Consistency is the strongest consistency model\nthat can be achieved if a system is designed to continue to be Available in the\nCAP sense. You can read more\n",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Causal_consistency"},"on Wikipedia"),"."),(0,i.kt)("p",null,"Causal Consistency is a model that is much simpler to work with when compared to\nEventual Consistency. Under Eventual Consistency, it seems like ",(0,i.kt)("em",{parentName:"p"},"anything")," is\nallowed to happen. With Causal Consistency, if one action happens before another,\nand can therefore potentially influence that other action, then those two actions must\nbe ordered that way for everyone. If two actions are totally unrelated, they can be\nordered any way the system chooses. By way of example:"),(0,i.kt)("p",null,"Imagine that you have two collections: Menus and Orders. First, you add a new\nitem to the menu, and then create an order that points to the new item. If these\ntwo independent actions were re-ordered by an eventually consistent system, some\ndevices could see that the menu item referenced in the order does not exist.\nCausal Consistency ensures that the menu item is added ",(0,i.kt)("em",{parentName:"p"},"before")," the order is\ncreated, regardless of the vagaries of networks, connections, and\nordering of messages. Transactional Causal Consistency means that we\ncan apply this constraint across any number of related changes, across\nmultiple documents, in multiple collections, as long as they are within the same\nApplication. This is a much simpler to understand model compared to Eventual\nConsistency, leading to fewer surprises."),(0,i.kt)("h3",{id:"paris---ust"},"PaRiS - UST"),(0,i.kt)("p",null,"This section gets technical on ",(0,i.kt)("em",{parentName:"p"},"how")," Big Peer provides Causally Consistent\nTransactions, and other properties, like fault tolerance, and scalability."),(0,i.kt)("p",null,"The key concept throughout, and the primitive on which Big Peer is built, is that of\nthe UST, the Universally Stable Timestamp. Along with some core architecture,\nthe UST is inspired by the paper ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/1902.09327"},"PaRiS: Causally Consistent Transactions with\nNon-blocking Reads and Partial Replication"),".\nThe paper describes a system that very closely matches Ditto's needs. The system\nis a database, one that is partitioned (or sharded) to allow storage of a great\ndeal of data, and replicated to provide fault tolerance (and better tail\nlatencies/work distribution.) PaRiS supports non-blocking reads in the\npast, and causally consistent transactions. The key ingredient is the UST."),(0,i.kt)("p",null,"In PaRiS every write transaction is given a unique timestamp. All\ntransactions that contain data for the same partitions will have a\ntimestamp that is ordered causally. Non-intersecting transactions can\nhave equal timestamps, as they have no causal relationship/order."),(0,i.kt)("p",null,"The key concept is that Transactions are ordered by Timestamp. Changes that have\na causal relationship express their order relationship through the order of\ntransaction timestamps. Transactions with no causal relationship can be ordered\nin any way. In the example above, as the change to the Menu collection\nhappens before the changes to the Orders collection. The first would have a\nlower transaction ID than the second (if not part of the ",(0,i.kt)("em",{parentName:"p"},"same")," transaction.)"),(0,i.kt)("p",null,"Before going into more details about Ditto's implementation, some clarification\non terms and concepts follows."),(0,i.kt)("h4",{id:"replicas"},"Replicas"),(0,i.kt)("p",null,"Replicas are independent copies of the same data, which provide fault tolerance\nand better performance. For example, if there exist three replicas and a disk fails, two\ncopies remain. If one or two replicas are unreachable due to network conditions, you\ncan still read from a reachable one. Replicas improve performance by providing more\ncapacity to serve reads. If you have three replicas you can balance reads across\nall three, each doing a third of the work. Data replication strategies (i.e.,\nhow data is replicated) have an effect on when you can read what."),(0,i.kt)("p",null,"As an initial look at the UST, imagine a database on a single machine with a\ntransaction log. Each transaction to be written goes into the log and is given a\nsequence number. When the transaction is committed, the sequence number can\nrepresent the current version of the database. For example, when transaction\nwith sequence number 1 is committed, the database is at Version 1. When\nthe second transaction commits, the database is at Version 2, and so on."),(0,i.kt)("p",null,"Let's walk through a replication example to understand how reads work. Say we have two replicas of our data, A and B. Replica\nA commits Transaction 1, and then sends it to Replica B, who also commits it.\nNow the database is at Version 1, and both replicas will return the same answer.\nBut what if Transaction 2 doesn't make it to Replica B? This can happen if\nthere is a brief network outage, or for some reason the message is delayed. So\nwhile message B is delayed, A has committed transactions 1 and 2, but B only\nhas committed Transaction 1. Since Ditto is causally consistent, it never\nblocks reads or writes. This means that a client can read even while replicas\nare in an inconsistent state."),(0,i.kt)("p",null,"If the system wishes to spread the read load equally, and a client reads from\nA, and after that reads from B, the client will see a non-consistent view of the\nworld, where time goes backwards between the first read, and the second.\nHowever, we could decide that since only Version 1 is committed on both replicas,\nthen the version of the database could be thought of as Version 1. This is the\nhighest transaction that is committed on both replicas: the universally stable\ntimestamp (UST). By enforcing reads to conform to the UST, clients reading from\neither replica will get a consistent view of the data."),(0,i.kt)("h4",{id:"versions"},"Versions"),(0,i.kt)("p",null,'The above scenario in "Replicas" suggests that we need to keep multiple versions\nof our data. If Transaction 1 changes documents A, B, and C, and Transaction 2\nchanges documents A, C, and F, BUT only one replica has stored both\ntransactions, then the database is at Version 1. We therefore need to have the\ndata for A and C at transactions 1 and 2, since if we want to provide a\nconsistent view of the data (one that does not go back in time) then we can only\nserve reads as of Version 1 at first, and then later as of Version 2.'),(0,i.kt)("p",null,"Big Peer keeps as many versions of each data item as it needs in order to provide\nconsistent reads. If this concerns you, skip ahead to garbage collection."),(0,i.kt)("p",null,'Note that Big Peer uses Ditto CRDTs as the data type, meaning all\nversions can be deterministically collapsed into one version, by\nmerging the CRDTs. In some cases a "version" is in only a ',(0,i.kt)("inlineCode",{parentName:"p"},"Diff")," and not a whole document."),(0,i.kt)("h4",{id:"partitions--shards"},"Partitions / Shards"),(0,i.kt)("p",null,"In order to evolve the conceptual model we can add in partitioning of\nthe data, or partial replication as it is called in the PaRiS\npaper. Often called Sharding, this is the practice of splitting up the\nkey space of a database, and assigning a subset of it to different\nservers. See Random Slicing below for details of HOW we do this in\nBig Peer."),(0,i.kt)("p",null,"Now we have replicas of the data, and we partition the data. Each storage node\nin Big Peer is responsible for one replica of a data partition. If we want to split\nour data across three partitions, and have two replicas of each item, then we can\ndeploy six servers, two in each partition."),(0,i.kt)("p",null,'Returning to our example in "Causally Consistent Transactions," imagine that\nthe documents in the Menus Collection is stored in Partition 1, and the Orders\nCollection in Partition 2, and that the change to Menus and Orders occurs\nin the same transaction, Transaction 1.'),(0,i.kt)("p",null,"This transaction contains documents that are stored in two different\npartitions, across a total of four locations (two replicas, two partitions)."),(0,i.kt)("p",null,"In order to store the data for this transaction it needs to be stored on\nall four servers. This is why the UST matters. If, by chance,\nBig Peer stores the Orders change document ",(0,i.kt)("em",{parentName:"p"},"before")," storing the\nMenus change document, and allow reads to always get the latest\nvalue, we can break the consistency constraint, and reference a menu item that doesn't exist."),(0,i.kt)("p",null,"A more complex example:"),(0,i.kt)("p",null,"If we have four transactions in flight, maybe all the servers have committed\nTransaction 1, half have committed Transaction 2, all have committed\nTransaction 3, and only two servers have committed Transaction 4. If we want to have\nconsistent read of the data, we have to read at the version that is stable at\nall servers: Transaction 1. Note: we can't say that Transaction 3 is\nstable, since it follows Transaction 2, which is not yet stable. Causal\nConsistency is all about the order of updates."),(0,i.kt)("h4",{id:"non-blocking-reads"},"Non-Blocking Reads"),(0,i.kt)("p",null,"When reading from Big Peer, you don't have to wait for the last write to\nbecome stable before reading. Instead, Big Peer is always able to return a\nversion of the data for the UST. Reading in the past is still causally\nconsistent, and it means that reads and writes proceed\nindependently. It also means that something is always available to be\nread (given one replica per-partition is reachable)\u2014a reasonable\ntrade-off."),(0,i.kt)("h4",{id:"read-your-own-writes"},"Read Your Own Writes"),(0,i.kt)("p",null,"In the PaRiS paper, the database clients must have a local cache of\ntheir own writes, so that they can always read their own writes. In\nDitto, the Small Peer clients are fully fledged partial replicas of the\ndatabase, and can ",(0,i.kt)("em",{parentName:"p"},"always")," read their own writes. For the HTTP API,\nwrites return a Timestamp at which the write is visible. A HTTP Read\nrequest can provide this timestamp to ensure Read-Your-Own-Writes\nsemantics."),(0,i.kt)("h3",{id:"the-log"},"The Log"),(0,i.kt)("p",null,"A core concept in Big Peer is the log. We use a transaction log to\npropagate updates to the database. In PaRiS, a two-phase commit process\nis used to negotiate an ",(0,i.kt)("a",{parentName:"p",href:"https://cse.buffalo.edu/tech-reports/2014-04.pdf"},"HLC"),"-based sequence\nnumber for each transaction. In Big Peer, we use the log to sequence\ntransactions. The sequence number for a Transaction in the log becomes\nthe Transaction Timestamp, which is what the UST\nreflects. The Transaction Timestamps in Big Peer form a total sequence,\nfrom ZERO (initial empty database version) on up. Each storage node\nconsumes from the log, and a transaction is stable when all nodes\nhave observed the transaction, those that own data in the\ntransaction having written that data durably."),(0,i.kt)("p",null,"At present our log is Kafka, as it suits our needs well. Though Kafka\nis at the heart of Big Peer, it is not a core architectural feature: any\nlog will do. At present, we use a single partition of a single topic,\nbut we can partition the log by Application and still maintain the\nsame consistency guarantees. When we do partition the log the properties are the\nsame, the throughput increases, and the UST becomes\na vector.  Developers can ",(0,i.kt)("a",{parentName:"p",href:"../guides/kafka/intro"},"register Kafka consumers"),"\nwhere Big Peer will deliver data change events that match a defined query -\nsimilar to how Small Peers can ",(0,i.kt)("inlineCode",{parentName:"p"},"observe")," queries to react to data changes."),(0,i.kt)("h3",{id:"storage-nodes"},"Storage Nodes"),(0,i.kt)("p",null,"Big Peer is split into Storage Nodes and Subscription Servers. The Storage Nodes\nare the database nodes, they run RocksDB as a local storage engine. A storage\nnode consumes the transaction log, commits data to disk, and gossips with the\nother storage nodes."),(0,i.kt)("h4",{id:"gossip---ust"},"Gossip - UST"),(0,i.kt)("p",null,"Each node gossips the highest transaction that it has committed. From\nthis gossip, any node can calculate what it considers to be the UST. If\nevery server gossips its local MAXIMUM committed transaction, then the\nUST is the MINIMUM of those MAXIMUMS. For example, in a three-node\ncluster:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Server 1 has committed Txn 10"),(0,i.kt)("li",{parentName:"ul"},"Server 2 has committed Txn 5"),(0,i.kt)("li",{parentName:"ul"},"Server 3 has committed Txn 7")),(0,i.kt)("p",null,'The UST is "5".'),(0,i.kt)("p",null,"NOTE: each server can have a different ",(0,i.kt)("em",{parentName:"p"},"view")," of the UST, depending on how\nlong it takes messages to be passed around. For example:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},'Server 1 has committed Txn 10, and has heard from Server 2 that it has\ncommitted Txn 4, and from Server 3 that it has committed Txn 6. Server 1\nthinks the UST is "4".'),(0,i.kt)("li",{parentName:"ul"},'Server 2 has committed Txn 5 and has heard from Server 1 that it has committed\nTxn 7, and from Server 3 that it has committed Txn 6. Server thinks the UST is\n"5"'),(0,i.kt)("li",{parentName:"ul"},'Server 3 has committed Txn 7 and has heard from Server 1 that it has committed\nTxn 9, and from Server 2 that it has committed Txn 3. Server thinks the UST is\n"3"')),(0,i.kt)("p",null,"But whatever the view of the UST, it reflects a causally consistent version of\nthe database that can be read."),(0,i.kt)("p",null,"When Big Peer is working, then the UST moves up. When Big Peer is quiescent the UST will\nbe the same on every node, and will reflect the last transaction produced by the\nlog."),(0,i.kt)("p",null,"The mechanism for gossip in Big Peer is the subject of future optimization work."),(0,i.kt)("h4",{id:"gossip---garbage-collection"},"Gossip - Garbage Collection"),(0,i.kt)("p",null,"Very similar to the UST is the Garbage Collection Timestamp. It works\nclosely with Read Transactions (below). The Cluster GC Timestamp\nrepresents the lowest Transaction Timestamp that must not be garbage\ncollected. The GC timestamp and the UST form a sliding window of\nversions over the database that represent the Timestamp versions at\nwhich a Causally Consistent query can be executed."),(0,i.kt)("p",null,"Document versions below the GC Timestamp can be garbage\ncollected. Garbage Collection is a periodic process that scans some\nsegment of the database, and rolls up, or merges all the versions\nbelow the GC timestamp, re-writing them as a single value. Thanks to\nDitto CRDTs, this leads to a deterministic outcome value for each\ndocument at each version."),(0,i.kt)("p",null,"Garbage Collection keeps the number of versions to a minimum, making\nreads more efficient, and reclaiming disk space."),(0,i.kt)("p",null,"The Garbage Collection Timestamp is calculated as the minimum active\nRead Transaction Timestamp across the cluster."),(0,i.kt)("h4",{id:"reading-and-read-transactions"},"Reading and Read Transactions"),(0,i.kt)("p",null,"Queries are handled by a coordinating node. Any node can coordinate a query, because every node has a local copy of the Partition Map, from the Cluster Configuration. As such, the coordinator can be chosen at random, or via some other load balancing heuristic."),(0,i.kt)("p",null,"The node will look at the\nquery and decide which partitions contain the data needed to answer\nthe query. At present, Big Peer shards data by Application AND Collection\n(however this can change in the future). The coordinator will assemble a\nlist of partitions needed to answer the query, and pick one replica\nfrom each partition. It picks the replica based on a\n",(0,i.kt)("a",{parentName:"p",href:"https://www.researchgate.net/profile/Benjamin-Satzger/publication/4339407_A_Lazy_Monitoring_Approach_for_Heartbeat-Style_Failure_Detectors/links/0912f50bcb4b32fc08000000/A-Lazy-Monitoring-Approach-for-Heartbeat-Style-Failure-Detectors.pdf"},"fault-detector"),",\npicking the replica least likely to be faulted. It sends the query to\neach replica, and merges and streams the results back to the caller."),(0,i.kt)("p",null,"The Coordinator issues the query to each partition with a predetermined timestamp.\nThis timestamp is usually the UST at the Coordinator, but can be any timestamp between the cluster Garbage Collection Timestamp and the UST."),(0,i.kt)("p",null,"When a node coordinates a Read Transaction, it locally holds some metadata in memory, indicating the value of the UST at the time the transaction began. This data is used to calculate the Local Garbage Collection Timestamp that the node gossips. The Local GC timestamp is the maximum transaction below the minimum read transaction. The GC timestamp proceeds monotonically upwards, as does the UST. When the query is complete, the Read Transaction is removed from memory, and the GC timestamp can rise."),(0,i.kt)("p",null,"A node that is not currently performing a Read Transaction will still gossip its view of the UST as the GC timestamp. This way progress can always be made."),(0,i.kt)("p",null,"In a quiescent cluster with no reads, the GC timestamp will equal the UST, and there will be exactly one version of each data item."),(0,i.kt)("h4",{id:"cluster-configurations-who-owns-what"},"Cluster Configurations: Who owns what?"),(0,i.kt)("p",null,"The details of the cluster: its size, shape, members, partitions,\nreplicas etc. are all encapsulated in a Cluster Configuration. When\nthere is a need to change a cluster, we create a new Cluster\nConfiguration and instruct Big Peer to transition from the Current\nConfiguration to the Next Configuration."),(0,i.kt)("p",null,"Everything discussed so far describes a static configuration of\npartitions and replicas. However, clusters must scale up and down, and\nfaulty nodes must be removed and replaced. Big Peer must support dynamic\nscaling without downtime, and it must do so while maintaining Causal\nConsistency, always accepting writes and serving reads."),(0,i.kt)("p",null,"Ideally, when a cluster is changed, there should be minimal data movement. That is, if\nwe grow the cluster, we want to only move the minimum amount of data necessary\nto the new nodes."),(0,i.kt)("p",null,"Before discussing Transitions in detail, it's helpful to look at how data is\nplaced in a Big Peer cluster, and for that we use Random Slicing."),(0,i.kt)("h4",{id:"random-slicing"},"Random Slicing"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://hpc.ac.upc.edu/PDFs/dir05/file004529.pdf"},"Random Slicing")," has been written about brilliantly in ",(0,i.kt)("a",{parentName:"p",href:"https://www.infoq.com/articles/dynamo-riak-random-slicing/"},"this\narticle")," by Scott Lystig-Fritchie, which motivates the WHY of Random\nSlicing as well as explaining the HOW. Here, we will briefly discuss Big Peer's implementation."),(0,i.kt)("p",null,"We made a decision to make this first version of Big Peer as simple as\npossible, and so we elected to keep our cluster shape and replica\nplacement very simple (though it is extensible and will get richer as\ntime allows or needs dictate)."),(0,i.kt)("p",null,'Each document in Big Peer has a key, or document ID which is made up of a\nNamespace (The Application (AppId) and the\ncollection) and an ID for the document. We hash a portion of this\nDocumentId (at present the Namespace) and that gives us a number. This\nnumber decides in which partition the data item lives. Our current\nhashing policy has the effect that data in the same Collection is\nco-located in the same partition, which makes queries in a single\nCollection more efficient. It may also lead to hot spots, but this can\nbe mitigated by either hashing more of the DocumentId (to split\nCollections), or inserting a layer of indirection that allows us to\nmap hot partitions to bigger nodes ("The Bieber problem": see the\npaper, or Scott\'s article for details.)'),(0,i.kt)("p",null,"As per the Random Slicing algorithm, we think of the keyspace as the range 0 to\n1","."," We take the ",(0,i.kt)("em",{parentName:"p"},"capacity")," of the cluster, and divide 1 by it. This determines\nhow much of the keyspace each partition owns."),(0,i.kt)("p",null,"In our initial, naive, implementation the capacity is the number of partitions we\nwish to have. We enforce an equal number of replicas per-partition, and thus all\nclusters are rectangular. E.g. 1","*","1, or 2","*","3, or 5","*","2, etc., where the first\nnumber is the number of partitions, and the second the number of replicas.\nRandom Slicing allows in future to have heterogeneous nodes, assigning the\ncapacity accordingly."),(0,i.kt)("p",null,"In the case that we want three partitions of two replicas, we say each\npartition takes 1/3 of the keyspace, or has 1/3 of the capacity."),(0,i.kt)("p",null,"Hashing a DocumentId then gives us a number that falls into the 1st,\n2nd or 3rd 1/3 of the keyspace, and that decides which partition\n",(0,i.kt)("em",{parentName:"p"},"owns")," that document."),(0,i.kt)("p",null,"We can transition from any configuration to any other, and we do this by slicing\nor coalescing partitions using the Cut-Shift algorithm from the Random Slicing\npaper."),(0,i.kt)("p",null,"The graphic below illustrates how this looks."),(0,i.kt)(h,{mdxType:"ImageHolder"},(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Random Slicing Cutshift",src:a(24213).Z}))),(0,i.kt)("p",null,"As the image shows, Partition 4 (P4) is made up of slices from P1, P2, and P3,\nthese three slices we call Intervals. They represent, in this case, two disjoint\nranges of the keyspace that P4 owns. A replica of P4 has two intervals, whereas\nP1 has a contiguous range and a single interval."),(0,i.kt)("p",null,"Our Random Slicing implementation is currently limited in that resources must be\nadded and removed in the cluster in units equal to the desired replication\nfactor. If you want to add a node, and your desired replication factor is two,\nyou must add two nodes. This is not a limit inherent in Random Slicing, but a\nchoice we made to speed up implementation. As Scott's article points out,\nRandom Slicing matches your keyspace to your storage capacity, but that is it!\nIt doesn't manage replica placement. More complex replica placement policies\nare coming, read Scott's article \ud83d\ude09"),(0,i.kt)("p",null,"In short, Random Slicing appears very simple, map capacity to the range\n0-1, and assign values to slices in that range. Cut-Shift is a great\nway to efficiently carve new smaller, partitions from slices of larger\nones, and coalesce smaller slices into larger partitions when Big Peer\nscales up or down."),(0,i.kt)("p",null,"Each storage node uses the Random Slicing partitioning information to\ndecide if it needs to store documents from any given\ntransaction. If the Random Slicing map says that Server One owns\nDocuments in the first Partition, then for each transaction Server One will\nstore Documents whose IDs hash to the first partition."),(0,i.kt)("h4",{id:"interval-maps---missed-transactions---backfill"},"Interval Maps - Missed Transactions - Backfill"),(0,i.kt)("p",null,"Each storage nodes keeps a local data structure, stored durably and\natomically with the document data, that records what transactions the\nnode has observed. The structure is called the ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap"),", and\nrepresents what has been observed, in what slices of the keyspace."),(0,i.kt)("p",null,'For example, if a server is responsible for an interval of the\nkeyspace that represents the first third of the keyspace, the server\n"splices" the observed transactions into the ',(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap")," at that\ninterval."),(0,i.kt)("p",null,"Imagine Server 1 is responsible for Interval 1, it receives transactions\n1..=100 from the log, it adds the data from those transactions to a local write\ntransaction with RocksDB. Then it splices the information into the IntervalMap,\nthat it has seen a block of transactions from 1..=100. We now say that the\n",(0,i.kt)("inlineCode",{parentName:"p"},"base")," for Interval 1 is ",(0,i.kt)("inlineCode",{parentName:"p"},"100"),". Now the server stores this updated\n",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap")," with the data in a write transaction to RocksDB."),(0,i.kt)("p",null,"Next the server receives transaction ",(0,i.kt)("inlineCode",{parentName:"p"},"150..=200")," from the log. Clearly the\nserver can detect that it has somehow missed transaction ",(0,i.kt)("inlineCode",{parentName:"p"},"101..=149"),". The server\ncan still observe and record the data from these new transactions, and splice\nthe information into the ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap"),". The ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap")," now has a ",(0,i.kt)("inlineCode",{parentName:"p"},"base")," of\n",(0,i.kt)("inlineCode",{parentName:"p"},"100")," and a ",(0,i.kt)("inlineCode",{parentName:"p"},"detached-range")," of ",(0,i.kt)("inlineCode",{parentName:"p"},"150..=200"),"."),(0,i.kt)("p",null,"Any server with any detached ranges can look in the Partition Map to see if it\nhas any peer replicas, and ask ",(0,i.kt)("em",{parentName:"p"},"them")," for the detached range(s). This is an\ninternal query in Big Peer. If a peer replica has some or all of the missing\ntransaction data, it will send it to the requesting server, who will splice the\nresults in the ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap"),", and write the data to disk. This way a server can\nrecover any data it missed, assuming at least one replica stored that data. We\ncall this Backfill."),(0,i.kt)("p",null,"Nodes gossip their ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMaps"),", this is how the UST is calculated, and how Backfill replicas can be chosen."),(0,i.kt)("p",null,'Read on down to "Missed/Lost Data" if you want to know how the cluster\ncontinues to make progress and function in the disastrous case that all\nreplicas miss a transaction.'),(0,i.kt)("p",null,"The ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap"),', gossip, Backfill, UST, Read Transactions, and the GC\ntimestamp all come together to facilitate "transitions", which is how Big Peer can\nscale up and down, while remaining operational, available, and consistent.'),(0,i.kt)("h2",{id:"transitions"},"Transitions"),(0,i.kt)("p",null,"Also mentioned in Scott's article on Random Slicing is the fact that\nRandom Slicing will not tell you how, or when, to move your data\naround if you want to go from one set of partitions to another."),(0,i.kt)("p",null,"In Big Peer we have the added problem that we must at all times remain\nCausally Consistent. Big Peer manages Transitions between configurations\nby leaning on those two primitives the UST and the GC Timestamp. The\nprocess is best explained with an example."),(0,i.kt)("p",null,"Using the diagram from the Random Slicing section, a walkthrough of the transition from the three-partition original cluster to the target four-partition cluster. In this case assume two replicas per partition, which means adding two new servers to the cluster."),(0,i.kt)("p",null,"There is a Current Config, that contains the intervals that make up the partitions 1, 2, and 3 mapped to the replicas for those partitions. The name ",(0,i.kt)("inlineCode",{parentName:"p"},"p1r1")," refers to the first replica of Partition 1, ",(0,i.kt)("inlineCode",{parentName:"p"},"p2r2")," the second replica of Partition 2, etc."),(0,i.kt)("p",null,"In the Current Config there are nodes ",(0,i.kt)("inlineCode",{parentName:"p"},"p1r1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"p1r2"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"p2r1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"p2r2"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"p3r1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"p3r2"),". Two new nodes are started, (",(0,i.kt)("inlineCode",{parentName:"p"},"p4r1"),", ",(0,i.kt)("inlineCode",{parentName:"p"},"p4r2"),"). A new Cluster Configuration is generated from the Current Configuration. This runs the Cut-Shift algorithm and produces a Next Configuration, with the partition map and intervals as-per the diagram above."),(0,i.kt)("p",null,"We store the Current Config, and the Next Config in a strongly consistent metadata store. Updating the metadata store causes the Current Config and Next Config file to be written to location on disk for each Big Peer Store node, and each node is signaled to re-read the configs."),(0,i.kt)("p",null,"The servers in ",(0,i.kt)("inlineCode",{parentName:"p"},"p1-p3")," are all in the Current Config, and the Next Config. The servers in ",(0,i.kt)("inlineCode",{parentName:"p"},"p4")," are only in the Next Config."),(0,i.kt)("p",null,"A server will consume from the log if it is in either config. Those in both configs will store data in all intervals they own in both configs. In our example each of the current config servers stores a subset of the current sub-interval of its current ownership in the next config. The new servers in ",(0,i.kt)("inlineCode",{parentName:"p"},"p4")," start to consume from the log at once, and gossip to all their peers in both configs."),(0,i.kt)("h3",{id:"backfill-again"},"Backfill, again"),(0,i.kt)("p",null,"For example, we start the new servers when the oldest transaction available on the log is ",(0,i.kt)("inlineCode",{parentName:"p"},"Txn Id 1000"),". They must Backfill from ",(0,i.kt)("inlineCode",{parentName:"p"},"0-1000")," from the owners of their intervals in the Current Configuration. They use the Current Configuration to calculate those owners, and the ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap"),"s from gossip to pick an owner to query for data no longer on the log. Recall that the UST is calculated from the ",(0,i.kt)("inlineCode",{parentName:"p"},"base")," of the ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap")," but these new servers (only part of the new config) do not contribute to the UST until they have Backfilled."),(0,i.kt)("h3",{id:"routing-and-a-ust-per-configuration"},"Routing, and a UST per-Configuration"),(0,i.kt)("p",null,"In the section on UST we described a scalar value, the Transaction Timestamp. In reality this value is a pair of the ",(0,i.kt)("inlineCode",{parentName:"p"},"ConfigurationId")," and the UST. The ",(0,i.kt)("inlineCode",{parentName:"p"},"ConfigurationId")," rises monotonically, the initial Configuration being ",(0,i.kt)("inlineCode",{parentName:"p"},"ConfigId 1"),", the second ",(0,i.kt)("inlineCode",{parentName:"p"},"ConfigId 2"),", etc."),(0,i.kt)("p",null,"This allows us to calculate a UST per-configuration. Before we began the transition the UST was ",(0,i.kt)("inlineCode",{parentName:"p"},"(1, 1000)"),". The UST may never go backwards (that would break Causal Consistency). After starting the new servers and notifying nodes about the Next Config, the UST in the Current Config is ",(0,i.kt)("inlineCode",{parentName:"p"},"(1, 1000)")," and in the Next Config is ",(0,i.kt)("inlineCode",{parentName:"p"},"(2, 0)"),". During this period of transition the nodes in ",(0,i.kt)("inlineCode",{parentName:"p"},"p4")," cannot be routed to for querying. Only nodes in the Current Config can coordinate queries, and these nodes decide what Configuration to use for Routing based on the USTs in each of the Current and Next Config. We call this the Routing Config. It is calculated. And like everything else in Big Peer, it progresses monotonically upwards."),(0,i.kt)("p",null,"After the new nodes have Backfilled, and after some period of gossip, the UST in the Next Config arrives at a value that is ",(0,i.kt)("inlineCode",{parentName:"p"},">=")," the UST in the current config","*"," so the servers in the Current Config will begin to Route queries using the Next Config. Recall that nodes gossip a GC timestamp that is based on active Read Transactions. A Read Transaction is identified by the Timestamp at which it began. For example ",(0,i.kt)("inlineCode",{parentName:"p"},"(1, 1000)")," is a Read Transaction that began at UST 1000 in the Current Configuration. When all the replicas in the Current Configuration are Routing with the next configuration, (e.g., the Cluster GC timestamp is in the Next Configuration, ",(0,i.kt)("inlineCode",{parentName:"p"},"(2, 1300)"),") the Transition is complete. Any of the nodes can store the Next Config into the Strongly Consistent metadata store as the Current Config. Each node is signaled, and eventually all will have a Current Config with ",(0,i.kt)("inlineCode",{parentName:"p"},"ConfigId 2"),", and will forget metadata related to ",(0,i.kt)("inlineCode",{parentName:"p"},"ConfigId 1"),". Furthermore, Garbage Collection will ensure that replicas drop data that they no longer own."),(0,i.kt)("p",null,"Throughout the transition, writes are processed, queries are executed, and the normal monotonic progress of the Cluster's UST and GC timestamp ensure that the new nodes can begin to store data at once, and will be used for query capacity as soon as they support Causally Consistent view of the data."),(0,i.kt)("p",null,"*","(there are details elided here about how we ensure that the Next Config makes progress and catches up with the current, whilst ensuring the cluster still moves forward)"),(0,i.kt)("h2",{id:"handling-failure"},"Handling Failure"),(0,i.kt)("p",null,"There are many failure scenarios in any distributed system. Big Peer leans heavily on a durable transaction log for many failure scenarios, and replicated copies of data for many others. Safety in Big Peer (bad things never happen) has been discussed at length above, in UST, and Transitions, and how Causally Consistent reads occur. Liveness, however, depends on every replica contributing to the UST. The UST (and GC timestamp) are calculated from gossip from ",(0,i.kt)("em",{parentName:"p"},"every")," node. If any node is down, partitioned by the network, slow, or in some other way broken, it impacts the progress of the cluster. Yes, Big Peer can still accept writes, and serve (ever staler) reads, but the UST won't rise, Transitions won't finish and GC will stall (leading to many versions on disk.)"),(0,i.kt)("p",null,"It is possible in future that we make some changes to how the UST is calculated, and use a quorum of nodes from a partition, or the single highest maximum transaction from a partition. The trade-off being that query routing becomes more complex, and in the event that the node that set the UST high then becomes unavailable...something has to give in terms of consistency. These trade-offs are mutable, we can re-visit them, we favoured safety in the current iteration of the design."),(0,i.kt)("p",null,"If some nodes are keeping the UST down, and slowing or halting progress, the bad node(s) can be removed."),(0,i.kt)("h3",{id:"bad-nodes"},"Bad Nodes"),(0,i.kt)("p",null,"In this first iteration of Big Peer we have the blunt, expedient tool available to us of killing and removing a node that is bad. The process is simple. Update the Current Config to remove the offending node(s) and signal the remaining servers. They will immediately no longer route to that node, use that node in their calculations, or listen to gossip from that node. This is fine as long as at least one node in each partition of the map is left standing."),(0,i.kt)("p",null,"As soon as the offending nodes are removed the data is under-replicated. At once add replacement nodes by performing a transition as above. For example, imagine ",(0,i.kt)("inlineCode",{parentName:"p"},"p1r2")," has become unresponsive. Remove it from the Current Config, create a Next Config with a new server to take the place of ",(0,i.kt)("inlineCode",{parentName:"p"},"p1r2"),", store the configs in the Strongly Consistent metadata store, and signal the nodes. The new node will begin to consume transactions and backfill, and the UST will rise etc."),(0,i.kt)("h3",{id:"missedlost-data"},"Missed/Lost Data"),(0,i.kt)("p",null,"As described in the first Backfill section, it is possible, with a long network incident and a short log retention policy, that some transactions are missed. If all the replicas for a partition miss some intersecting subset of transactions, that data has been missed, and it is lost. This should ",(0,i.kt)("em",{parentName:"p"},"never")," happen. If it does, we don't want to throw away the Big Peer cluster, and all the good data. Progress must still be made. In this case each replica of the partition understands from the ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap"),"s that some transaction ",(0,i.kt)("inlineCode",{parentName:"p"},"T")," has been missed. After doing a strongly consistent read of the metadata store, to check that no server in the next config exists that may have the data, the replicas agree unilaterally to pretend that really they did store this data, and they splice it into their ",(0,i.kt)("inlineCode",{parentName:"p"},"IntervalMap"),"s. The UST rises, and progress is made."),(0,i.kt)("p",null,"It is essential to understand this is a disaster scenario, and not business as usual, but disasters happen, and they should be planned for. We do everything we can to never lose data, including a replicated durable transaction log with a long retention policy."),(0,i.kt)("h3",{id:"subscription-servers"},"Subscription Servers"),(0,i.kt)("p",null,"These are soft-state servers that act as other Peers to Small Peers. They speak\nthe Ditto replication protocol. Small Peers connect to the subscription server,\nand based on their subscription, the SubscriptionServer will replicate data.\nTaking from the Small Peer device data that Big Peer has not seen, and sending to the Small Peer\ndevice data Big Peer has that the Small Peeer device subscribes to, and has not seen."),(0,i.kt)("p",null,"Subscription Servers also provide an HTTP API for non-Small Peer clients."),(0,i.kt)("h4",{id:"document-cache"},"Document Cache"),(0,i.kt)("p",null,"In order to not be required to query Big Peer for all data requested by Small Peers, all the time, the Subscription Server maintains a sophisticated, causally consistent, in-memory cache of a data. The data it chooses to cache is based on the Subscriptions of the Small Peers connected to it. By routing devices to a Subscription Server by AppId, we increase the likelihood that the devices have an overlapping set of Subscriptions and share common data in the cache."),(0,i.kt)("p",null,'The document cache takes data from the mesh clients and puts it on the log as transactions. It also consumes from the log, so that it can keep the data in the cache up to date, without querying Big Peer. Any documents that it observes on the log, that are potentially of interest to the cache, must first be "Upqueried" from Big Peer to populate the cache. As a cache becomes populated Upqueries decrease in size and number.'),(0,i.kt)("p",null,"As clients disconnect, if any data is no longer required in the cache, it is eventually garbage collected away."),(0,i.kt)("h2",{id:"what-next"},"What Next?"),(0,i.kt)("p",null,"Big Peer is approaching beta. Some customers are already putting production workloads into Big Peer. Big Peer is far from done. We'd like to focus on stability, performance, and features. As well as those mentioned above (gossip, sophisticated resourcing/partitioning) we have the following in progress."),(0,i.kt)("h3",{id:"cdc"},"CDC"),(0,i.kt)("p",null,"Completing the cycle of data in Big Peer is CDC (",(0,i.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Change_data_capture"},"Change Data Capture"),"). Work in progress where each transaction produces a Change Data Message containing the type of change (e.g., insert, delete, update) and the details of the change. CDC is a way for customer to react to data changes that occur from the mesh or elsewhere, or even to keep external legacy databases in sync with Big Peer."),(0,i.kt)("p",null,"Data from CDC is available from Kafka. Developers can ",(0,i.kt)("a",{parentName:"p",href:"../guides/kafka/intro"},"register Kafka consumers")," where Big Peer will deliver data change events that match a defined query - similar to how Small Peers can ",(0,i.kt)("inlineCode",{parentName:"p"},"observe")," queries to react to data changes."),(0,i.kt)("p",null,'We also provide webhooks to enable delivery of data within Big Peer into other systems or to build server-side logic that reacts to data change events - such as performing data aggregations that write back into Big Peer or triggering an email to a user based off an event from a Small Peer. These data change events fit into "serverless" patterns and will work with any "functions-as-a-service" (FaaS) systems, such as AWS Lambda or others.'),(0,i.kt)("p",null,"Care is being taken to ensure the delivery of these events are reliable. Endpoints will be able to persist a unique marker that corresponds to the event, and later restart events from that same marker onward so that events are not missed during periods of interruption."),(0,i.kt)("h3",{id:"time-series"},"Time Series"),(0,i.kt)("p",null,'In addition to storing mutable Documents backed by CRDTs, Big Peer recently added support for immutable Time Series data. We have a basic Time Series API in Big Peer and we\'re adding Time Series to the Small Peer. Developers can utilize these APIs to match data in their application, such as blending Documents from a companion mobile app alongside sensor Time Series data produced via an embedded or "IoT" device. Watch this space.'),(0,i.kt)("h3",{id:"http-api"},"HTTP API"),(0,i.kt)("p",null,"For more information on how to use the HTTP API, see the ",(0,i.kt)("a",{parentName:"p",href:"https://docs.ditto.live/http/installation"},"HTTP documentation"),"."),(0,i.kt)("h3",{id:"eventual-consistency-mode"},"Eventual Consistency Mode"),(0,i.kt)("p",null,'Multi-model databases are becoming more popular. We\'d like to add an "Eventual Consistency Mode" to Big Peer, where the overhead of Causal Consistency is not needed.'),(0,i.kt)("h3",{id:"improved-queries-and-indexing"},"Improved Queries and Indexing"),(0,i.kt)("p",null,"We'd like to expand the types of queries offered, such as cross-collection joins, in addition, to offer flexible indexing for improved query performance."),(0,i.kt)("h3",{id:"summary"},"Summary"),(0,i.kt)("p",null,"Big Peer is multi-tenant, distributed, causally consistent CRDT database,\nbuilt by Ditto, and operated by Ditto. When creating Big Peer our view\nwas always towards which architectural fundamentals give us the\nprimitives we need to support the properties we want."),(0,i.kt)("p",null,"Big Peer is under active development. We expect to expand and iterate and to\noptimize. Big Peer has been written as software that Ditto can ship and support\ncustomers deploying and running themselves, running it as a service at Ditto is\nonly the first part of the story."))}c.isMDXComponent=!0},11748:(e,t,a)=>{var n={"./locale":89234,"./locale.js":89234};function i(e){var t=s(e);return a(t)}function s(e){if(!a.o(n,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return n[e]}i.keys=function(){return Object.keys(n)},i.resolve=s,e.exports=i,i.id=11748},24213:(e,t,a)=>{"use strict";a.d(t,{Z:()=>n});const n=a.p+"assets/images/cutshift-17d225642fc7e916bda2ae752449ef38.png"},83081:(e,t,a)=>{"use strict";a.d(t,{Z:()=>n});const n=a.p+"assets/images/diagram-0668f99c916da479bb921499c46fee1c.png"}}]);